<!--
author: W3layouts
author URL: http://w3layouts.com
License: Creative Commons Attribution 3.0 Unported
License URL: http://creativecommons.org/licenses/by/3.0/
-->
<!DOCTYPE html>
<link rel="shortcut icon" href="https://d1nhio0ox7pgb.cloudfront.net/_img/g_collection_png/standard/256x256/fire.png">
<html lang="en">
<head>
		<title>I'm SunnerLi</title>
		<!-- for-mobile-apps -->
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="keywords" content="Work Responsive web template, Bootstrap Web Templates, Flat Web Templates, Android Compatible web template, 
Smartphone Compatible web template, free webdesigns for Nokia, Samsung, LG, SonyEricsson, Motorola web design" />

    <script>
        addEventListener("load", function () {
            setTimeout(hideURLbar, 0);
        }, false);

        function hideURLbar() {
            window.scrollTo(0, 1);
        }
    </script>
	
	<!-- css files -->
    <link href="css/bootstrap.css" rel='stylesheet' type='text/css' /><!-- bootstrap css -->
    <link href="css/style.css" rel='stylesheet' type='text/css' /><!-- custom css -->
    <link href="css/fontawesome-all.css" rel="stylesheet"><!-- fontawesome css -->
	<!-- //css files -->
	
	<!-- google fonts -->
	<link href="//fonts.googleapis.com/css?family=Mukta:200,300,400,500,600,700,800&amp;subset=devanagari,latin-ext" rel="stylesheet">
	<link href="//fonts.googleapis.com/css?family=Niramit:200,200i,300,300i,400,400i,500,500i,600,600i,700,700i&amp;subset=latin-ext,thai,vietnamese" rel="stylesheet">
	<!-- //google fonts -->
	
</head>
<body>

<!-- header -->
<header class="bg-white py-1">
	<div class="container">
		<nav class="navbar navbar-expand-lg navbar-light">
			<h1>
				<!-- <a class="navbar-brand" href="index.html"><i class="fab fa-python"></i> Sunner</a> -->
			</h1>
			<!-- <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
				<span class="navbar-toggler-icon"></span>
			</button> -->

			<div class="collapse navbar-collapse" id="navbarSupportedContent">
				<ul class="navbar-nav ml-lg-4 mr-auto">
						<!-- <li class="nav-item">
							<a class="nav-link" href="index.html">Home</a>
						</li> -->

						<li class="nav-item dropdown">
							<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
							  Paper Notes
							</a>
							<div class="dropdown-menu" aria-labelledby="navbarDropdown">
								<a class="dropdown-item" href="paper_note_2019_single.html">2019 (Single)</a>
								<a class="dropdown-item" href="about.html">2018 (Single)</a>
								<a class="dropdown-item" href="paper_note_2018_multiple.html"> 2018 (Multiple)</a>
							</div>
						</li>
						
						<li class="nav-item dropdown">
							<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
									Repository
							</a>
							<div class="dropdown-menu" aria-labelledby="navbarDropdown">
								<a class="dropdown-item" href="project.html">Project</a>
								<a class="dropdown-item" href="paper_reimplement.html"> Paper Idea Implementation</a>
							</div>
						</li>

						<li class="nav-item dropdown">
							<a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
							  Books
							</a>
							<div class="dropdown-menu" aria-labelledby="navbarDropdown">
								<a class="dropdown-item" href="https://sunnerli.gitbooks.io/test-book/content/">Machine Learning Foundation</a>
								<a class="dropdown-item" href="https://sunnerli.gitbooks.io/test-book-also/content/">Machine Learning Technique</a>
								<a class="dropdown-item" href="https://sunnerli.gitbooks.io/prml/content/">Machine Learning (NCTU)</a>
								<a class="dropdown-item" href="https://sunnerli.gitbooks.io/test-book4/content/"> The Introduction of NLP</a>
							</div>
						</li>

						  <li class="nav-item">
							<a class="nav-link" href="contact.html">Contact</a>
						  </li>
				</ul>
				<div class="header-right">
					<!-- <a href="signin.html" class="signin mr-4"> Sign in <i class="fas fa-sign-in-alt"></i></a> -->
					<a href="index.html" class="contact">Home</a>
				</div>
			</div>
		</nav>
	</div>
</header>
<!-- //header -->

<!-- banner -->
<section class="inner-banner">
	<div class="container">
	</div>
</section>
<!-- //banner -->


<!-- about -->
<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="heading mb-5">
			<h3 class="head text-center">Paper Notes</h3>
			<p class="my-3 head text-center"> 本頁列出了我2019看過paper的相關中文筆記，最新看過的paper會被擺在愈上面，但每一個項目只和一篇論文有關。</p>
		</div>
	</div>
</section>

<!-- 新增templete -->
<!--
<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> </h3>       
				<p class="mb-4">   </p>        
				<a href="#">Paper</a>             
				<a href="#">Note</a>           
				<a href="#">Code</a>     
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/WTrUdst.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section> -->

<!-- 從這邊開始往下新增 -->

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> 【隨筆】Image-to-Image Translation via Group-wise Deep Whitening-and-Coloring Transformation

				</h3>       
				<p class="mb-4"> 針對已知的I2I方法，要把資訊從examplar轉移到input image，大多使用一些normalization的技巧，譬如AdaIN(我猜是MUNIT)。廣義來說，這包刮了whitening和coloring兩個過程，然而此兩個過程計算過程很耗時，因此本paper提出一種end-to-end的方法來有效率地整合whitening和coloring到I2I中。

				</p>        
				<a href="https://arxiv.org/abs/1812.09912">Paper</a>             
				<a href="https://hackmd.io/vdn8UphdTsS_nmXzDFRi8w">Note</a>              
				<a href="https://github.com/WonwoongCho/GDWCT">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/c1ShjdZ.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> 【隨筆】TraVeLGAN: Image-to-image Translation by Transformation Vector Learning

				</h3>       
				<p class="mb-4"> 本篇想解決的問題是I2I，特別的是，作者並沒有使用cycle consistency！反倒是引入了一個siamese network來限制latent space，這樣的做法可以讓I2I作用在domain gap較大的情況下，而不僅是結構類似但紋理不同的場景。

				</p>        
				<a href="https://arxiv.org/abs/1902.09631">Paper</a>             
				<a href="https://hackmd.io/dz8j1P1nTsq5l17cQZMYcQ">Note</a>              
				<a href="https://github.com/KrishnaswamyLab/travelgan">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/4OZ5hcU.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Towards Instance-level Image-to-Image Translation

				</h3>       
				<p class="mb-4"> 以前做I2I方法(MUNIT or DRIT)的缺點是，操作影像的content，但僅調整至target domain的global style，也就是沒有針對物體的資訊去微調，這樣轉出來的效果不算精準。因此本篇paper提出instance-aware image-to-image translation (INIT)，變成instance-level的I2I，其優點有兩者：第一，instance-level objective loss可以幫助generator針對多樣物品做正確轉換；第二，INIT的style具備了spatial information，不再僅是global style了！

				</p>        
				<a href="https://arxiv.org/abs/1905.01744">Paper</a>             
				<a href="https://hackmd.io/S3GldHWAQFa1IaXpZpyVFA">Note</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/1sRxeor.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> 【隨筆】CollaGAN: Collaborative GAN for Missing Image Data Imputation

				</h3>       
				<p class="mb-4"> 本篇想要解決的問題，是做missing data的資料補齊，作者提出Collaborative Generative Adversarial Network (CollaGAN)，將imputation的問題轉換成Image-to-image translation的問題來解決。

				</p>        
				<a href="https://arxiv.org/abs/1901.09764">Paper</a>             
				<a href="https://hackmd.io/6odj3PzuSwuxJWHlUZkmMg">Note</a>              
				<a href="https://github.com/jongcye/CollaGAN_CVPR">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/Yi1qS5t.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> 【隨筆】mixup: Beyond Empirical Risk Minimization

				</h3>       
				<p class="mb-4"> DNN雖然強大，但她習慣去記憶訓練資料(overfitting)，且對於adversarial example過於敏感。在此研究中，作者提出mixup，在訓練時用資料對的combination來訓練，label和圖片都有採用此技巧。實驗證實mixup可以有效防止網路去記憶訓練資料的label，且對於adversarial example有更強抵抗能力。

				</p>        
				<a href="https://arxiv.org/abs/1710.09412">Paper</a>             
				<a href="https://hackmd.io/-nHurryyQ0Wlo2OQLSANKA">Note</a>              
				<a href="https://github.com/facebookresearch/mixup-cifar10">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/lbWpZPK.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation

				</h3>       
				<p class="mb-4"> 本篇paper提出一個新的unsupervised I2I方法，稱為UGATIT，概念是結合learnable normalization和attention module，和傳統的I2I不同，UGATIT可以透過新提出的AdaLIN，應付較大的structure、texture change。

				</p>        
				<a href="https://arxiv.org/abs/1907.10830">Paper</a>             
				<a href="https://hackmd.io/kQQT1GqHTjSEGZ0oDnVwKw">Note</a>              
				<a href="https://github.com/taki0112/UGATIT">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/Y08EWMJ.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> 【隨筆】fast patch-based style transfer of arbitrary style

				</h3>       
				<p class="mb-4"> 作者提出style swap，為一種local matching的方法，作者額外使用80000張自然影像跟80000張畫作來訓練inverse network，實驗證明inverse network跟直接用style swap做BP出來的效果差不多！上圖是做出來的效果。

				</p>        
				<a href="https://arxiv.org/abs/1612.04337">Paper</a>             
				<a href="https://hackmd.io/0fW8et4jR4C1oIB2ta2miw">Note</a>              
				<a href="https://github.com/rtqichen/style-swap">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/dibAFD6.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> 【隨筆】Arbitrary Style Transfer with Style-Attentional Networks

				</h3>       
				<p class="mb-4"> 本篇paper提出style-attentional network (SANet)來做style transfer，和其他方法相比，更能將local style patterns嵌入到content image的semantic spatial distribution之中。作者更提出identity loss和multi-level feature embedding來更加維持content structure。

				</p>        
				<a href="https://arxiv.org/abs/1812.02342">Paper</a>             
				<a href="https://hackmd.io/MVSfC88MRsKaqpus-CriUw">Note</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/wIjbDI0.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Learning Linear Transformations for Fast Arbitrary Style Transfer

				</h3>       
				<p class="mb-4"> 先前做任意風格轉換的演算法，通常計算複雜度都很高，而且不容易捕捉到畫風之間的變異性，也會產生一些artifact。本篇paper提出一種transformation matrix的形式來解決arbitrary style transfer的問題，這種彈性的架構在轉換的過程中也會將content給保留住。

				</p>        
				<a href="https://arxiv.org/abs/1808.04537">Paper</a>             
				<a href="https://hackmd.io/M5coMnlDRoiTszkvAeZ3jA">Note</a>              
				<a href="https://github.com/sunshineatnoon/LinearStyleTransfer">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/FQPQnYR.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> 【隨筆】Photorealistic Style Transfer via Wavelet Transforms

				</h3>       
				<p class="mb-4"> 本篇paper提出WCT2，主要概念是提出一個wavelet corrected transfer，將wavelet transform自然的嵌入到NN中，並且不需要任何post-processing。

				</p>        
				<a href="https://arxiv.org/abs/1903.09760">Paper</a>             
				<a href="https://hackmd.io/XR_BGBsTSKWv_7VPXDuwbA">Note</a>              
				<a href="https://github.com/clovaai/WCT2">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/CqBkcAa.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> 【隨筆】Avatar-Net: Multi-scale Zero-shot Style Transfer by Feature Decoration

				</h3>       
				<p class="mb-4"> AvatarNet的關鍵在於style decorator，透過他來對content feature和style feature做alignment。上圖是Avatar-Net做出來的效果。

				</p>        
				<a href="https://arxiv.org/abs/1805.03857">Paper</a>             
				<a href="https://hackmd.io/51sgpyu0SreZVKzu1nOiVw">Note</a>              
				<a href="https://github.com/LucasSheng/avatar-net">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/8WCJ7ks.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis

				</h3>       
				<p class="mb-4"> 本篇paper提出VAE的改良，稱為自省變分編碼器 - introspective VAE (IntroVAE)，用來做高解析度image synthesizing。IntroVAE融合了VAE和GAN的思想，但卻沒有discriminator的存在！僅使用inference model (encoder)來辨別樣本的真實性，測試在CelebA上可以產生幾乎SOTA的效果！

				</p>        
				<a href="https://arxiv.org/abs/1807.06358">Paper</a>             
				<a href="https://hackmd.io/N7JMgOsoSu2AEN2OGATriA">Note</a>              
				<a href="https://github.com/dragen1860/IntroVAE-Pytorch">Code</a>              
				<a href="https://github.com/SunnerLi/IntroVAE-PyTorch">My Implementation</a>
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/6xXDb54.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Wasserstein Divergence for GANs

				</h3>       
				<p class="mb-4"> 目前GAN中WGAN表現的非常不錯，理論推導也很完備，然而在實作上，去逼近k-Lipschitz連續的限制來近似Wasserstein-1 metric是非常有挑戰性的！本paper提出Wasserstein divergence (W-div)的概念，可看作是W-met的放寬限制版本，不需要k-Lipschitz連續的限制。

				</p>        
				<a href="https://arxiv.org/abs/1712.01026">Paper</a>             
				<a href="https://hackmd.io/IyddpCjYSc2mwXdCslaScg">Note</a>              
				<a href="https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan_div/wgan_div.py">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/v0qGwis.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Improving the Improved Training of Wasserstein GANs: A Consistency Term and Its Dual Effect

				</h3>       
				<p class="mb-4"> 本篇paper從WGAN出發，提出一個新方法，把Lipschitz continuity的限制綁在訓練過程中。

				</p>        
				<a href="https://arxiv.org/abs/1803.01541">Paper</a>             
				<a href="https://hackmd.io/IPzjmspJS9SY-3IeGmV63Q">Note</a>              
				<a href="https://github.com/Randl/improved-improved-wgan-pytorch">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/dtoclJQ.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow

				</h3>       
				<p class="mb-4"> 本篇paper是想要改善GAN中discriminator會反饋沒有資訊梯度的議題，提出variational discriminator bottleneck (VDB)，概念是限制observation和discriminator internal representation之間的mutual information。可以想像成就是在discriminator internal representation中加噪聲，讓discriminator別學那麼快。

				</p>        
				<a href="https://arxiv.org/abs/1810.00821">Paper</a>             
				<a href="https://hackmd.io/tFc4YO8LThqdblSiJmx-6A">Note</a>              
				<a href="https://github.com/akanimax/Variational_Discriminator_Bottleneck/">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/3XzfmcM.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Which Training Methods for GANs do actually Converge?

				</h3>       
				<p class="mb-4"> 本篇paper論述：對於GAN訓練，資料分布必須是absolute continuity，如果不是，即使是WGAN這類的模型都可能會無法收斂，沒辦法走到equilibrium point。作者近一步提出一種regularization的策略，並討論instance noise和zero-centered gradient panalties都可讓收斂發生。作者證明即使data distribution和generator分布在低維流形中，簡單的添加regularization term的作法仍能讓GAN走到local convergence！

				</p>        
				<a href="https://arxiv.org/abs/1801.04406">Paper</a>             
				<a href="https://hackmd.io/RtZcKioBQHihziUeqTeTpQ">Note</a>              
				<a href="https://github.com/LMescheder/GAN_stability">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/k7rhGhj.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> [隨筆] Progressive GAN

				</h3>       
				<p class="mb-4"> 在本篇paper中，作者提出progressive growing的技巧來訓練GAN，並提出一種簡單的方法，來增加generated image的變異性。此外，作者提出一些技巧，來改善G和D的不健康競爭關係。

				</p>        
				<a href="https://arxiv.org/abs/1710.10196">Paper</a>             
				<a href="https://hackmd.io/6mBPg1TMT4ac6VUvtHkGpw">Note</a>              
				<a href="https://github.com/nashory/pggan-pytorch">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/47dsZYw.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> LIA: Latently Invertible Autoencoder with Adversarial Learning

				</h3>       
				<p class="mb-4"> 本篇想做的是跟PIONEER類似，在deep generative model存在著兩個問題：VAE做variational inference表現沒有很好、GAN則沒有encoding sample的能力。本paper提出Latently Invertible Autoencoder (LIA)來解決此問題，在encoder-decoder的架構向，隱藏了一個deep invertible network和inverse deep invertible network，用來把feature distribution投射到一個 𝑞 中，並近似prior。two-stage stochasticity-free training是訓練LIA的方法。實驗表明做在FFHQ上有著很好的效果！

				</p>        
				<a href="https://arxiv.org/abs/1906.08090">Paper</a>             
				<a href="https://hackmd.io/2aVlC1yJTniqT1CIbcYsGw">Note</a>              
				<a href="https://github.com/zhujiapeng/LIA">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/vL90kl2.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Pioneer Networks: Progressively Growing Generative Autoencoder

				</h3>       
				<p class="mb-4"> 本篇提出了一種新穎的generative autoencoder模型。GAN雖然能夠產生高品質圖像，但卻沒有reconstruction的能力；本篇融合了最近被提出了ptogressive技巧，提出了PIONEER，在訓練時完全不需要discriminator，在CelebA inference的部分取得了SOTA，可以想像成是progressive + VAE做在image generation中！

				</p>        
				<a href="https://arxiv.org/abs/1807.03026">Paper</a>             
				<a href="https://hackmd.io/ejyRFabEQGCVgs2GGPgOzg">Note</a>              
				<a href="https://github.com/heljakka/pioneer">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/qj5wJo9.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> [隨筆] StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks

				</h3>       
				<p class="mb-4"> 本篇paper想解決的問題，是想要針對給定的text description，產生high-quality images。作者提出StackGAN來產生256x256的照片。整個計算過程分成兩個階段，第一個階段產生物件的primitive shape和大致的colors，第二階段則產生較photo-realistic details。另外作者還提出conditioning augmentation的技巧，來產生較為平滑的latent conditioning manifold。

				</p>        
				<a href="https://arxiv.org/abs/1612.03242">Paper</a>             
				<a href="https://hackmd.io/XDLQiXx1Qiuwl-A5-OqPrw">Note</a>              
				<a href="https://github.com/hanzhanggit/StackGAN">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/u4otoHn.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>


<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> ELEGANT: Exchanging Latent Encodings with GAN for Transferring Multiple Face Attributes

				</h3>       
				<p class="mb-4"> 對於face attribute transfer的問題，目前還有三個爭議：(1) 目前都是從latent space直接產生圖像，而沒辦法從examplars產生圖像 (2) 沒辦法同時改變多個attribute (3) 產生的圖像有artifact。本篇paper提出交換latent encoding的思想，並以此提出新的架構：ELEGANT(Exchanging Latent Encoding with GAN for Transferring multiple face attributes)，在這樣的思想下，所有的attribute都可被encode，且具備disentanglement的特性。

				</p>        
				<a href="https://arxiv.org/abs/1803.10562">Paper</a>             
				<a href="https://hackmd.io/B8SFZ_10RBKYFD42HCeFrA">Note</a>              
				<a href="https://github.com/Prinsphield/ELEGANT">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/mBw7i1L.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Attention-aware Multi-stroke Style Transfer

				</h3>       
				<p class="mb-4"> 以前style transfer的方法沒辦法整合content image和stylized image中的visual attention spatial distribution，或是透過不同程度的brush strokes來繪製圖像。本文則提出一種attention-aware multi-stroke style transfer來同時解決這兩個問題，將attention map中的salient characteristic給整合進來，加入不同spatial region中的multiple stroke pattern，來達到更好的渲染效果！

				</p>        
				<a href="https://arxiv.org/abs/1901.05127">Paper</a>             
				<a href="https://hackmd.io/CnYAlsF2TOCcgvIo3wTDSQ">Note</a>              
				<a href="https://github.com/JianqiangRen/AAMS">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/tgDYpuB.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> A Style-Aware Content Loss for Real-time HD Style Transfer

				</h3>       
				<p class="mb-4"> 針對風格轉換，以前的工作大多是用pre-trained CNN來比較風格學得好不好，譬如VGG，但這會受到ImageNet BBOX的bias的影響，因為ImageNet本身是做辨識的，不是做風格轉換的，把影像分成多個box對衡量藝術並沒有直接關係，本篇paper提出以audo-encoder為基礎的風格轉換網路，並結合GAN和style-aware content loss，使得網路可以捕捉到風格影響內容的微妙本質，譬如說鐘塔稍微有特色的扭曲，上圖最右邊的column。

				</p>        
				<a href="https://arxiv.org/abs/1807.10201">Paper</a>             
				<a href="https://hackmd.io/MmCxsV7HS9OsLvjRjFG2LQ">Note</a>              
				<a href="https://github.com/CompVis/adaptive-style-transfer">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/R7Af10e.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> WarpGAN: Automatic Caricature Generation

				</h3>       
				<p class="mb-4"> 本篇論文提出WarpGAN，來對輸入的臉部影像產生caricatures（漫畫效果），同時，網路也產生一些可修改的control points，讓臉部照片可以被warp成caricature，除了可客製化如何產生face caricature外，還同時保有identity的特性，實驗結果說明WarpGAN可以產生多樣的caricatures！效果如上圖所示。

				</p>        
				<a href="https://arxiv.org/abs/1811.10100">Paper</a>             
				<a href="https://hackmd.io/-jfQoBtTTfqRgYPitHgdpw">Note</a>              
				<a href="https://github.com/seasonSH/WarpGAN">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/nkUjMOa.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> WarpGAN: Automatic Caricature Generation

				</h3>       
				<p class="mb-4"> 本篇論文提出WarpGAN，來對輸入的臉部影像產生caricatures（漫畫效果），同時，網路也產生一些可修改的control points，讓臉部照片可以被warp成caricature，除了可客製化如何產生face caricature外，還同時保有identity的特性，實驗結果說明WarpGAN可以產生多樣的caricatures！效果如上圖所示。

				</p>        
				<a href="https://arxiv.org/abs/1811.10100">Paper</a>             
				<a href="https://hackmd.io/-jfQoBtTTfqRgYPitHgdpw">Note</a>              
				<a href="https://github.com/seasonSH/WarpGAN">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/nkUjMOa.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Few-Shot Adversarial Learning of Realistic Neural Talking Head Models

				</h3>       
				<p class="mb-4"> 本篇paper想要解決的問題是neural talking head model，白話來說就是產生一個說話的人頭video，但特別的地方在於可以針對few-shot或one-shot來作客製化，這樣一來收斂速度會很快！

				</p>        
				<a href="https://arxiv.org/abs/1905.08233">Paper</a>             
				<a href="https://hackmd.io/sKuG1xW9SUeU6mvzaPhMsA">Note</a>              
				<a href="https://github.com/grey-eye/talking-heads">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/jvk7hty.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Deep Flow-Guided Video Inpainting

				</h3>       
				<p class="mb-4"> 本篇想解決的是domain adaptation的問題。以前大部分的方法是想要學習domain invariant representation。本篇的思路也是類似，但使用了duplex adversarial discriminator (DupGAN)。整體網路包含了三個部分：encoder, generator和兩個discriminators。Generator在生成影像時會conditon on一個domain code，來生成不同domain的影像。在這樣的框架下，不僅可以學到domain invariant特性，還可以保留category information

				</p>        
				<a href="https://arxiv.org/abs/1905.02884">Paper</a>             
				<a href="https://hackmd.io/I1iWYy6gTfOzI_VrtsNWfQ">Note</a>              
				<a href="https://github.com/nbei/Deep-Flow-Guided-Video-Inpainting">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/ZXHV8En.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Duplex Generative Adversarial Network for Unsupervised Domain Adaptation Lanqing

				</h3>       
				<p class="mb-4"> 本篇想解決的是domain adaptation的問題。以前大部分的方法是想要學習domain invariant representation。本篇的思路也是類似，但使用了duplex adversarial discriminator (DupGAN)。整體網路包含了三個部分：encoder, generator和兩個discriminators。Generator在生成影像時會conditon on一個domain code，來生成不同domain的影像。在這樣的框架下，不僅可以學到domain invariant特性，還可以保留category information

				</p>        
				<a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Duplex_Generative_Adversarial_CVPR_2018_paper.html">Paper</a>             
				<a href="https://hackmd.io/HUaT8crbRpOtyRvWm4NCIw">Note</a>              
				<a href="http://vipl.ict.ac.cn/view_database.php?id=6">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/PmSlzbe.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> MoCoGAN: Decomposing Motion and Content for Video Generation

				</h3>       
				<p class="mb-4"> 本篇paper提出MoCoGAN來解決video generation。基本思路是，一張影像可以被拆解成content和motion的組合，content代表畫面中的物體，motion代表物體移動性。MoCoGAN會把一系列的random vector投射成一系列的圖片。當content固定時，motion則可看成是一個隨機過程，實驗證明MoCoGAN的確可以產生固定content不同motion的影片！上圖是示意圖。

				</p>        
				<a href="https://arxiv.org/abs/1707.04993">Paper</a>             
				<a href="https://hackmd.io/50-9-rtzTRO4jDNfHX9G7A">Note</a>              
				<a href="https://github.com/sergeytulyakov/mocogan">Code</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/RTPjz2v.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Features for Multi-Target Multi-Camera Tracking and Re-Identification

				</h3>       
				<p class="mb-4"> 這篇paper解決了兩個問題：MTMCT(multi-target multi-camera tracking)和Person Re-ID(re-identification)的問題，作者提出了創新的adaptive weighted triplet loss和hard-identity mining兩個技巧，並測試在DukeMTMC資料集中，取得了更好的Re-ID和MTMCT score！

				</p>        
				<a href="https://arxiv.org/abs/1803.10859">Paper</a>             
				<a href="https://hackmd.io/7VwFjWRtQxSFKNmO7H-usg">Note</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/sSLinAx.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Scaling and Benchmarking Self-Supervised Visual Representation Learning

				</h3>       
				<p class="mb-4"> 在本篇paper中，作者重新審視2個有名的self-supervised learning方法(Jigsaw和Colorization)，並擴展訓練影像至1億張照片。作者時坐在不同的task上，包刮object detection、3D surface normal estimation和visual navigation，實驗證明某些任務的效能超過supervised learning！但作者也表明目前的self-supervised method並沒辦法充分學習到資料中的high level semantic representation。

				</p>        
				<a href="https://arxiv.org/abs/1905.01235">Paper</a>             
				<a href="https://hackmd.io/S6AZMMjsQKCBeOmrEe0pDA">Note</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/mYINJqC.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Features for Multi-Target Multi-Camera Tracking and Re-Identification

				</h3>       
				<p class="mb-4"> 這篇paper解決了兩個問題：MTMCT(multi-target multi-camera tracking)和Person Re-ID(re-identification)的問題，作者提出了創新的adaptive weighted triplet loss和hard-identity mining兩個技巧，並測試在DukeMTMC資料集中，取得了更好的Re-ID和MTMCT score！

				</p>        
				<a href="https://arxiv.org/abs/1803.10859">Paper</a>             
				<a href="https://hackmd.io/7VwFjWRtQxSFKNmO7H-usg">Note</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/sSLinAx.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> O-GAN: Extremely Concise Approach for Auto-Encoding Generative Adversarial Networks

				</h3>       
				<p class="mb-4"> 本paper提出O-GANs(Orthogonal Generative Adversarial Networks)，透過新增加一個額外的損失項，使得discriminator變成一個很好的encoder，且這樣的設計也不會降低discriminator的自由度。本篇paper也是最簡單的方法，讓GAN擁有auto-encoding的能力！

				</p>        
				<a href="https://arxiv.org/abs/1903.01931">Paper</a>             
				<a href="https://hackmd.io/wC1G-iMkSf2SkE-rdgw97A">Note</a>              
				<a href="https://github.com/bojone/o-gan">Code</a>
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/QgtsYYy.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> AutoAugment: Learning Augmentation Strategies from Data

				</h3>       
				<p class="mb-4"> 目前做data argumentation的方法都是自己手動設計的。本paper提出AutoAugment，可自動搜索最適合的augmentation policies。一個augmentation policies可能包含許多sub-policies，而sub-policy包含了兩個operation，每個operation可以是旋轉、平移、扭曲等動作，在外加上機率和大小。透過作者提出了搜索演算法，挑選出能讓validation acc最高的policies來當作結果！白話一點說就是：用搜索演算法來取代手動想data augmentation的方式！

				</p>        
				<a href="https://arxiv.org/abs/1805.09501">Paper</a>             
				<a href="https://hackmd.io/SZMmecX3QxC0eRDtXyVHfw">Note</a>              
				<a href="https://github.com/DeepVoltaire/AutoAugment">Code</a>
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/479Vn4Q.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Billion-scale semi-supervised learning for image classification

				</h3>       
				<p class="mb-4"> 本篇paper提出一個semi-supervised learning pipeline，來解決image classification的問題。其概念是基於teacher-student paradigm。這樣的做法可以針對target architecture提升很多performance！

				</p>        
				<a href="https://arxiv.org/abs/1905.00546">Paper</a>             
				<a href="https://hackmd.io/Ap2WPnvJRgGkRlD2say86A">Note</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/MYSTPPW.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Play as You Like: Timbre-enhanced Multi-modal Music Style Transfer

				</h3>       
				<p class="mb-4"> 本paper使用了MUNIT的架構來做music-to-music transformation。為了抓初不同樂器的音色和樂器相關演奏細節，輸入混合了4種特徵，包括MFCC、spectral difference、spectrogram和spectral envelop，並使用RaGAN。最後做在三種不同的風格樂器上，包括piano solo、guitar solo和string quartet。

				</p>        
				<a href="https://arxiv.org/pdf/1811.12214.pdf">Paper</a>             
				<a href="https://hackmd.io/M5iRJzrbTsiQiboFXYWX0g">Note</a>              
				<a href="https://github.com/ChienYuLu/Play-As-You-Like-Timbre-Enhanced-Multi-modal-Music-Style-Transfer">Code</a>
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/SKgUbwI.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Learning Features of Music from Scratch

				</h3>       
				<p class="mb-4"> 本篇論文提出了一個新的音樂資料集：MusicNet，而訓練的時候是定義一個multi-label note prediction，藉此來衡量這個資料集是否夠豐富！

				</p>        
				<a href="https://arxiv.org/abs/1611.09827">Paper</a>             
				<a href="https://hackmd.io/hlO68bITSUyKdUOuMJr8Fg">Note</a>              
				<a href="https://github.com/jthickstun/pytorch_musicnet">Code</a>
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/aLliN3j.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Music Thumbnailing via Neural Attention Modeling of Music Emotion

				</h3>       
				<p class="mb-4"> music thumbnailing的目標是希望找到一個較短且連續的音樂片段，使得這個片段最能代表一首歌。概念是結合emotion recognition來解決music chorus selection的問題。作者引入LSTM和attention layer到CNN中，來做music emotion classification；attention layer則會對每個3秒鐘的chunk評估他的重要性，用31000首歌和相對應的emotion labels來訓練。實驗結果表明有80%的歌曲會把副歌當作是thumbnails。

				</p>        
				<a href="http://mac.citi.sinica.edu.tw/~yang/pub/huang17apsipa.pdf">Paper</a>             
				<a href="https://hackmd.io/WHdadI_pQIiWHkBB9oFuaA">Note</a>              
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/Jr2wWRv.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Event Localization in Music Auto-tagging

				</h3>       
				<p class="mb-4"> 本篇想解決的是frame-level的music auto-tagging，即給定一個music clip，我們想知道他的attribute，包刮樂器種類、風格和其他聲學屬性。

				</p>        
				<a href="http://mac.citi.sinica.edu.tw/~yang/pub/liu16mm.pdf">Paper</a>             
				<a href="https://hackmd.io/yTTD0RxaQgWvuHELbLj_Wg">Note</a>           
				<a href="https://github.com/ciaua/clip2frame">Code</a>     
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/ScJ5UtV.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Hit Song Prediction for Pop Music by Siamese CNN with Ranking Loss

				</h3>       
				<p class="mb-4"> 這篇paper想要解的問題是hit song prediction。就是在一首歌被發行以前，預測他是否會很紅。傳統方法是把他看成classification(紅或不紅）或regression（紅的分數）的問題，這篇則看成是ranking的問題。作者使用了multi-objective siamese CNN，並結合了Euclidean loss和pairwise ranking loss，來學習不同歌曲之間的排名關係。根據實驗顯示，搭配上A/B sampling可以比其他baseline獲得更高的準確度！

				</p>        
				<a href="https://arxiv.org/abs/1710.10814">Paper</a>             
				<a href="https://hackmd.io/g4eKoSD6QSWalLP2FDBx9A">Note</a>           
				<a href="https://github.com/OckhamsRazor/HSP_CNN">Code</a>     
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/KzX9T24.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Transferring GANs: generating images from limited data

				</h3>       
				<p class="mb-4"> 本paper探討的問題是，把domain adaptation的技巧用在GAN的image generation。結果顯示利用pre-trained network的knowledge可以縮短訓練時間，而且如果target data很少的情況下，產生的圖像品質也可大幅度提升！

				</p>        
				<a href="https://arxiv.org/pdf/1805.01677">Paper</a>             
				<a href="https://hackmd.io/DyNQf_RkSLCQ6XH9__HB0Q?view">Note</a>           
				<a href="https://github.com/yaxingwang/Transferring-GANs">Code</a>     
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/1Kaj9Dv.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Unsupervised Image Super-Resolution using Cycle-in-Cycle Generative Adversarial Networks

				</h3>       
				<p class="mb-4"> 本篇paper對於super resolution有著不同的看法：low/high resolution的影像配對其實是無法獲得的！在傳統的方法中，低解析度影像其實已經退化，參雜一些noise和blurring。本文提出Cycle-in-cycle的架構來解決unpair super resolution的問題！整個計算過程分成兩步。第一，會先將noisy且blurry的圖片投射到noise-free low-resolution空間中，第二，會把這個intermediate image透過pre-trained deep model投射到high-resolution空間。

				</p>        
				<a href="https://arxiv.org/abs/1809.00437">Paper</a>             
				<a href="https://hackmd.io/0Oh7ymreT4mh9rlUqEfZCw">Note</a>           
				<a href="https://github.com/Junshk/CinCGAN-pytorch">Code (unofficial)</a>     
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/L0AoXFk.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> MSG-GAN: Multi-Scale Gradient GAN for Stable Image Synthesis

				</h3>       
				<p class="mb-4"> GAN惡名昭彰難用的其中一個原因，是因為訓練過程並不穩定。因為從discriminator傳遞過來的gradient，到generator已經幾乎不具有資訊；本paper提出Multi-scale gradient GAN (MSG-GAN)來解決這個問題，允許discriminator中不同流向、不同尺度的gradient直接傳到generator中，作者實驗在CIFAR10、Oxford102 flowers和CelebA-HQ中皆取得不錯的效果！

				</p>        
				<a href="https://arxiv.org/abs/1903.06048">Paper</a>             
				<a href="https://hackmd.io/6GaiqNanSYi0Kx16MVNwiQ?view">Note</a>           
				<a href="https://github.com/akanimax/BMSG-GAN">Code</a>     
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/g2LvsxV.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Maximum Classifier Discrepancy for Unsupervised Domain Adaptation

				</h3>       
				<p class="mb-4"> 本篇paper提出一個unsupervised domain adaptation的方法。傳統上在解這個問題時，通常使用一個domain classifier，並試圖讓source feature和target feature越近越好，但這有兩個缺點：第一，這類的方法並沒有使用task-specific decision boundary的資訊；第二，不同domain有著不同的domain-specific characteristic，一昧地要讓兩個distribution互相靠近是很困難的！因此本文提出了一個新的方法，試圖利用並最大化task-specific decision boundary，最大化兩個classifier 的output來達成目標！

				</p>        
				<a href="https://arxiv.org/abs/1712.02560">Paper</a>             
				<a href="https://hackmd.io/3MmvzkBcQdyJr2rozCDN-Q?view">Note</a>           
				<a href="https://github.com/mil-tokyo/MCD_DA">Code</a>     
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://raw.githubusercontent.com/mil-tokyo/MCD_DA/master/docs/overview.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Self-Supervised Feature Learning for Semantic Segmentation of Overhead Imagery

				</h3>       
				<p class="mb-4"> 這篇paper想處理的對象是航空影像(overhead image)，然而針對這樣的影像做semantic segmentation是個巨大的挑戰，因為根本沒有GT，而且和地面照有著很大的domain gap。作者提出一種self-supervised的方法來對overhead imagery做semantic segmentation，並透過adversarial training有了更好的pre-trained feature。

				</p>        
				<a href="http://bmvc2018.org/contents/papers/0345.pdf">Paper</a>             
				<a href="https://hackmd.io/WIGVOhcyQmmzZoVjzygVLQ">Note</a>           
				<a href="https://github.com/suriyasingh/Self-supervision-for-segmenting-overhead-imagery">Code</a>     
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/fm6f4is.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Semantic Image Synthesis with Spatially-Adaptive Normalization

				</h3>       
				<p class="mb-4"> 本篇提出了spatially-adaptive normalization，進而結合進pix2pixHD，形成一個新的模型為GauGAN，用來解決image synthesize的問題。傳統的CNN大多使用疊起來的(conv, BN, ReLU)運算組合，但這樣的做法會把輸入的semantic information"洗掉"，因此作者提出spatially-adaptive版本的可學習轉換，來解決這個爭議。上圖是一個簡單的結果，如果先求出最左邊column的latent representation，再透過SPADE生成圖片的話，style就會有所不同。

				</p>        
				<a href="https://arxiv.org/abs/1903.07291">Paper</a>             
				<a href="https://hackmd.io/r-u6WR8fRO6uPK7HSC3nZQ">Note</a>           
				<a href="">Code (TBC)</a>     
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/N2zZNei.jpg" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> GANimation: Anatomically-aware Facial Animation from a Single Image

				</h3>       
				<p class="mb-4"> 先前已有研究來做臉部表情合成(facial expression synthesis)，最知名的例子便是StarGAN，但僅能學出離散型別的表情(因為在StarGAN中attribute是離散變量)。本篇paper提出基於Action Units(AU)的臉部表情合成方法，AU定義了人臉表情，且是分布在一個低微度且連續的manifold中，作者指出僅需使用有AU標註的影像進行非監督式訓練，即可達到更好的效果，可渲染出更多樣的表情，包括臉部肌肉的彈性度。

				</p>        
				<a href="https://arxiv.org/pdf/1807.09251.pdf">Paper</a>             
				<a href="https://hackmd.io/uzvBHQ6HRyWcjJLI345a5g">Note</a>           
				<a href="https://github.com/albertpumarola/GANimation">Code</a>     
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/3mEf8z8.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Identifying Analogies Across Domains

				</h3>       
				<p class="mb-4"> 雖然現在unsupervised identifying analogy的研究已經可以達到不錯的效果，但轉換圖像的visual fidelity並不足夠識別在另外一個domain中的matching sample。本篇paper提出AN-GAN，採用一種名為matching-by-synthesis的思想，把轉換的過程分成兩部：domain alignment和learning the mapping function。

				</p>        
				<a href="https://openreview.net/pdf?id=BkN_r2lR-">Paper</a>             
				<a href="https://hackmd.io/Ao5qK-nURTK6vx-lNT1UTw">Note</a>               
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/EtlwxhI.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>


<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> A Closer Look at Few-shot Classification
				</h3>       
				<p class="mb-4"> 由於不同的few-shot algorithm有不同的implementation detail，且比較的方式也不太一樣，因此很難公平的比較哪一個方法比較好。本篇paper提出：第一，公平的分析與比較得知，如果用的backbone network較深，且domain difference較少的話，不同的方法都可顯著的降低classification gap；第二，稍微對網路做一些改良，便可在mini-ImageNet和CUB兩個資料集更加提升SOTA；最後，對於cross-domain的few shot classification問題，提出一個新的實驗設定。

				</p>        
				<a href="https://openreview.net/pdf?id=HkxLXnAcFQ">Paper</a>             
				<a href="https://hackmd.io/jmipjbf0TseG_lxd8baWQA?view">Note</a>           
				<a href="https://github.com/wyharveychen/CloserLookFewShot">Code</a>     
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/39AVmGL.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> DeepFool: a simple and accurate method to fool deep neural networks
				</h3>       
				<p class="mb-4"> 對於以前adversarial attack的方法，並沒有一個很精準的方法可以計算很小的perturbations。本篇paper提出DeepFool，可以較有效率並精準的計算最小的perturbation，以欺騙classifier。

				</p>        
				<a href="https://arxiv.org/pdf/1511.04599.pdf">Paper</a>             
				<a href="https://hackmd.io/jwfnZWTkQ86R4UNfTwfk2w?view">Note</a>           
				<a href="https://github.com/LTS4/DeepFool/tree/master/Python">Code</a>     
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/Rz5rdgh.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Image Super-Resolution as a Defense Against Adversarial Attacks
				</h3>       
				<p class="mb-4"> CNN對於含有adversarial noise pattern的輸入可能較容易被混淆，本paper提出一種防禦機制來緩解adversarial pertubation造成的性能丟失問題。在本研究中，image restoration network會把off-the-manifold adversarial sample拉回natural image manifold中，且此方法不需要修改classifier的參數，也不需要額外的過程去偵測哪些圖片是adversarial example。

				</p>        
				<a href="https://arxiv.org/abs/1901.01677">Paper</a>             
				<a href="https://hackmd.io/zfJ_8XSSQ8iIrEJX3nAjXw">Note</a>           
				<a href="https://github.com/aamir-mustafa/super-resolution-adversarial-defense">Code</a>     
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/SY7nZka.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning
				</h3>       
				<p class="mb-4"> 先前做image inpainting的研究，做出來的結果普遍很平滑或是模糊。本論文提出EngeConnect，是一種兩階段的對抗是模型，由edge generator和image completion network組成，網路會把幻想出來的edge當作prior來渲染，實驗在CelebA和Places2資料集中取得了SOTA。

				</p>        
				<a href="https://arxiv.org/abs/1901.00212">Paper</a>             
				<a href="https://hackmd.io/9npdnYG2QzGJtthZIFPKiA?view">Note</a>           
				<a href="https://github.com/knazeri/edge-connect">Code</a>     
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/BsQm5c3.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section>

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> CCNet: Criss-Cross Attention for Semantic Segmentation
                </h3>       
				<p class="mb-4">  long-range dependency在visual understanding裡也同樣重要，但傳統CNN的respected field是有限的，因此本文提出Criss-Cross Network (CCNet)。網路中含有criss-cross attention module，該module收集了對每個pixel十字架路徑上的contextual information。比起He的non-local network，CCNet不僅實作上較為GPU-friendly（快了11倍），計算上也較有效率（FLOPs為15%） </p>        
				<a href="https://arxiv.org/abs/1811.11721">Paper</a>             
				<a href="https://hackmd.io/QDIdfKtFS7-5864v6Xm2kA">Note</a>           
				<a href="https://github.com/speedinghzl/CCNet">Code</a>     
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/4sNp6i4.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section> 

<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> Adversarial Discriminative Domain Adaptation
                </h3>       
				<p class="mb-4">  對抗式學習可以被拿來改善recognition task的performance，消除domain shift或dataset bias造成的性能損失。先前的研究可分為兩者討論：

                    Prior generative approach：讓generator的部份資料分佈儘量一致，但這類型的方法沒辦法被用在domain shift很大的情況
                    Prior discriminative approach: 讓discriminator儘量保持一致，這類型的方法大多是強迫tie weight(share weight)，也沒有用到GAN 的概念
                    本研究結合了discriminative model, weight sharing和GAN loss的想法，提出了Adversarial Discriminative Domain Adaptation框架(ADDA)，是一種非監督式的domain adaptation方法。 </p>        
				<a href="https://arxiv.org/abs/1702.05464">Paper </a>             
				<a href="https://hackmd.io/3Fa2I-coRQqBMH42XDo0QA">Note</a>           
				<a href="https://github.com/erictzeng/adda">Code</a>     
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/yLsW4Nx.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
    </div>
    
<section class="about-us py-5">
	<div class="container py-md-3">
		<div class="row bottom-grids">
			<div class="col-lg-7 col-md-1 px-sm-0">
				<h3 class="my-3"> InstaGAN: Instance-aware Image-to-Image Translation
                </h3>       
				<p class="mb-4"> 在image-to-image translation的任務中，先前的方法會在較為複雜的考驗中失敗，物體的形狀很難被保留，例如有多個instance要轉換的場景。本研究利用instance information，提出了instance-aware GAN (InstaGAN)，並引入了context preserving loss，迫使網路在target instance以外的範圍學習對等映射(輸入=輸出)

                </p>        
				<a href="https://arxiv.org/abs/1812.10889">Paper </a>             
				<a href="https://hackmd.io/QYDwy8mBTweaZY2DSdeYaQ">Note</a>           
				<a href="https://github.com/sangwoomo/instagan">Code</a>     
			</div>
			<div class="col-lg7 offset-sm-1 px-lg-1">
					<br></br>
					<br></br>
				<img src="https://i.imgur.com/q9xY61z.png" height=350 width=350 alt="" class="img-fluid"/>
			</div>
		</div>
	</div>
</section> 



<!-- copyright -->
<section class="copy-right bg-light py-4">
	<div class="container">
		<div class="row">
			<div class="col-lg-7 col-md-9">
				<p class="">© 2018 Work. All rights reserved | Design by
					<a href="http://w3layouts.com"> W3layouts.</a>
				</p>
			</div>
			<div class="col-lg-5 col-md-3">
				<ul class="social-iconsv2 agileinfo d-flex">
					<li>
						<a href="#">
							<i class="fab fa-facebook-square"></i>
						</a>
					</li>
					<li>
						<a href="#">
							<i class="fab fa-twitter-square"></i>
						</a>
					</li>
					<li>
						<a href="#">
							<i class="fab fa-google-plus-square"></i>
						</a>
					</li>
					<li>
						<a href="#">
							<i class="fab fa-linkedin"></i>
						</a>
					</li>
				</ul>
			</div>
		</div>
	</div>
</section>
<!-- copyright -->

    <!-- js -->
    <script src="js/jquery-2.2.3.min.js"></script>
    <script src="js/bootstrap.js"></script>
    <!-- //js -->
	
	<!-- dropdown nav -->
    <script>
        $(document).ready(function() {
            $(".dropdown").hover(
                function() {
                    $('.dropdown-menu', this).stop(true, true).slideDown("fast");
                    $(this).toggleClass('open');
                },
                function() {
                    $('.dropdown-menu', this).stop(true, true).slideUp("fast");
                    $(this).toggleClass('open');
                }
            );
        });
    </script>
    <!-- //dropdown nav -->

	<script src="js/smoothscroll.js"></script><!-- Smooth scrolling -->


</body>
</html>